import time
import psutil
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Callable
from functools import wraps
from dataclasses import dataclass, asdict
from pathlib import Path
import json
import matplotlib.pyplot as plt

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    execution_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    gpu_usage_percent: Optional[float] = None
    throughput: Optional[float] = None  # ì²˜ë¦¬ëŸ‰ (samples/second)
    accuracy: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None
    f1_score: Optional[float] = None

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ"""
    
    def __init__(self, save_results: bool = True, results_dir: str = "benchmark_results"):
        """
        ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.save_results = save_results
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.benchmark_history: List[Dict[str, Any]] = []
        
        logger.info("ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def measure_performance(self, func_name: str = None):
        """
        í•¨ìˆ˜ ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°
        
        Args:
            func_name: í•¨ìˆ˜ ì´ë¦„ (Noneì´ë©´ ìë™ ì„¤ì •)
        """
        def decorator(func: Callable):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ì‹œì‘ ì‹œê°„ ë° ë¦¬ì†ŒìŠ¤ ì¸¡ì •
                start_time = time.time()
                start_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
                start_cpu = psutil.cpu_percent(interval=0.1)
                
                try:
                    # í•¨ìˆ˜ ì‹¤í–‰
                    result = func(*args, **kwargs)
                    
                    # ì¢…ë£Œ ì‹œê°„ ë° ë¦¬ì†ŒìŠ¤ ì¸¡ì •
                    end_time = time.time()
                    end_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
                    end_cpu = psutil.cpu_percent(interval=0.1)
                    
                    # ë©”íŠ¸ë¦­ ê³„ì‚°
                    execution_time = end_time - start_time
                    memory_usage = end_memory - start_memory
                    cpu_usage = (start_cpu + end_cpu) / 2
                    
                    # GPU ì‚¬ìš©ëŸ‰ ì¸¡ì • (nvidia-ml-py í•„ìš”)
                    gpu_usage = self._get_gpu_usage()
                    
                    # ë©”íŠ¸ë¦­ ê°ì²´ ìƒì„±
                    metrics = PerformanceMetrics(
                        execution_time=execution_time,
                        memory_usage_mb=memory_usage,
                        cpu_usage_percent=cpu_usage,
                        gpu_usage_percent=gpu_usage
                    )
                    
                    # ê²°ê³¼ ì €ì¥
                    name = func_name or func.__name__
                    self._save_benchmark_result(name, metrics, args, kwargs)
                    
                    logger.info(f"ğŸ” {name} ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ:")
                    logger.info(f"  ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
                    logger.info(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}MB")
                    logger.info(f"  CPU ì‚¬ìš©ë¥ : {cpu_usage:.1f}%")
                    if gpu_usage:
                        logger.info(f"  GPU ì‚¬ìš©ë¥ : {gpu_usage:.1f}%")
                    
                    return result
                    
                except Exception as e:
                    logger.error(f"í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    raise
                    
            return wrapper
        return decorator
    
    def benchmark_image_processing(self, processor, image_paths: List[str], 
                                 num_iterations: int = 10) -> Dict[str, Any]:
        """
        ì´ë¯¸ì§€ ì²˜ë¦¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        
        Args:
            processor: ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° ê°ì²´
            image_paths: í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            num_iterations: ë°˜ë³µ íšŸìˆ˜
            
        Returns:
            ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        logger.info(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {len(image_paths)}ê°œ ì´ë¯¸ì§€, {num_iterations}íšŒ ë°˜ë³µ")
        
        all_times = []
        memory_usage = []
        
        for iteration in range(num_iterations):
            start_time = time.time()
            start_memory = psutil.virtual_memory().used / 1024 / 1024
            
            for image_path in image_paths:
                try:
                    # ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤í–‰
                    processed = processor.preprocess(image_path)
                except Exception as e:
                    logger.warning(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {image_path} - {str(e)}")
                    continue
            
            end_time = time.time()
            end_memory = psutil.virtual_memory().used / 1024 / 1024
            
            iteration_time = end_time - start_time
            iteration_memory = end_memory - start_memory
            
            all_times.append(iteration_time)
            memory_usage.append(iteration_memory)
            
            logger.info(f"ë°˜ë³µ {iteration + 1}/{num_iterations}: {iteration_time:.3f}ì´ˆ")
        
        # í†µê³„ ê³„ì‚°
        avg_time = np.mean(all_times)
        std_time = np.std(all_times)
        avg_memory = np.mean(memory_usage)
        throughput = len(image_paths) / avg_time
        
        results = {
            'function': 'image_processing_benchmark',
            'num_images': len(image_paths),
            'num_iterations': num_iterations,
            'avg_execution_time': avg_time,
            'std_execution_time': std_time,
            'avg_memory_usage_mb': avg_memory,
            'throughput_images_per_sec': throughput,
            'all_times': all_times,
            'memory_usage': memory_usage
        }
        
        # ê²°ê³¼ ì €ì¥
        if self.save_results:
            self._save_detailed_results('image_processing_benchmark', results)
        
        logger.info("ğŸ“Š ì´ë¯¸ì§€ ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        logger.info(f"  í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.3f}Â±{std_time:.3f}ì´ˆ")
        logger.info(f"  í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {avg_memory:.2f}MB")
        logger.info(f"  ì²˜ë¦¬ëŸ‰: {throughput:.2f} ì´ë¯¸ì§€/ì´ˆ")
        
        return results
    
    def benchmark_template_generation(self, generator, original_path: str, 
                                    mask_path: str, num_iterations: int = 5) -> Dict[str, Any]:
        """
        í…œí”Œë¦¿ ìƒì„± ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        
        Args:
            generator: í…œí”Œë¦¿ ìƒì„±ê¸° ê°ì²´
            original_path: ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ
            mask_path: ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ê²½ë¡œ
            num_iterations: ë°˜ë³µ íšŸìˆ˜
            
        Returns:
            ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        logger.info(f"í…œí”Œë¦¿ ìƒì„± ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {num_iterations}íšŒ ë°˜ë³µ")
        
        all_times = []
        quality_scores = []
        
        for iteration in range(num_iterations):
            output_path = f"temp_template_{iteration}.png"
            
            start_time = time.time()
            
            try:
                # í…œí”Œë¦¿ ìƒì„±
                template = generator.generate_template(original_path, mask_path, output_path)
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ìˆëŠ” ê²½ìš°)
                if hasattr(generator, 'get_inpainting_quality_score'):
                    import cv2
                    original = cv2.imread(original_path)
                    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
                    quality = generator.get_inpainting_quality_score(original, template, mask)
                    quality_scores.append(quality)
                
            except Exception as e:
                logger.warning(f"í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                continue
            
            end_time = time.time()
            iteration_time = end_time - start_time
            all_times.append(iteration_time)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            temp_path = Path(output_path)
            if temp_path.exists():
                temp_path.unlink()
            
            logger.info(f"ë°˜ë³µ {iteration + 1}/{num_iterations}: {iteration_time:.3f}ì´ˆ")
        
        # í†µê³„ ê³„ì‚°
        avg_time = np.mean(all_times)
        std_time = np.std(all_times)
        avg_quality = np.mean(quality_scores) if quality_scores else None
        
        results = {
            'function': 'template_generation_benchmark',
            'num_iterations': num_iterations,
            'avg_execution_time': avg_time,
            'std_execution_time': std_time,
            'avg_quality_score': avg_quality,
            'all_times': all_times,
            'quality_scores': quality_scores
        }
        
        # ê²°ê³¼ ì €ì¥
        if self.save_results:
            self._save_detailed_results('template_generation_benchmark', results)
        
        logger.info("ğŸ“Š í…œí”Œë¦¿ ìƒì„± ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        logger.info(f"  í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.3f}Â±{std_time:.3f}ì´ˆ")
        if avg_quality:
            logger.info(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.3f}")
        
        return results
    
    def _get_gpu_usage(self) -> Optional[float]:
        """GPU ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        try:
            import pynvml
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            return util.gpu
        except:
            return None
    
    def _save_benchmark_result(self, func_name: str, metrics: PerformanceMetrics, 
                             args: tuple, kwargs: dict) -> None:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        result = {
            'timestamp': time.time(),
            'function_name': func_name,
            'metrics': asdict(metrics),
            'args_count': len(args),
            'kwargs_keys': list(kwargs.keys())
        }
        
        self.benchmark_history.append(result)
        
        if self.save_results:
            # ê°œë³„ ê²°ê³¼ íŒŒì¼ ì €ì¥
            filename = f"{func_name}_{int(time.time())}.json"
            with open(self.results_dir / filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
    
    def _save_detailed_results(self, benchmark_name: str, results: Dict[str, Any]) -> None:
        """ìƒì„¸ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        results['timestamp'] = time.time()
        
        filename = f"{benchmark_name}_{int(time.time())}.json"
        with open(self.results_dir / filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {filename}")
    
    def generate_performance_report(self, output_path: Optional[str] = None) -> str:
        """
        ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            output_path: ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ
            
        Returns:
            ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ
        """
        if not self.benchmark_history:
            logger.warning("ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        
        # í•¨ìˆ˜ë³„ ì„±ëŠ¥ í†µê³„ ê³„ì‚°
        function_stats = {}
        for record in self.benchmark_history:
            func_name = record['function_name']
            metrics = record['metrics']
            
            if func_name not in function_stats:
                function_stats[func_name] = {
                    'execution_times': [],
                    'memory_usage': [],
                    'cpu_usage': []
                }
            
            function_stats[func_name]['execution_times'].append(metrics['execution_time'])
            function_stats[func_name]['memory_usage'].append(metrics['memory_usage_mb'])
            function_stats[func_name]['cpu_usage'].append(metrics['cpu_usage_percent'])
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report_lines = [
            "# OCR AI ëª¨ë“ˆ ì„±ëŠ¥ ë¦¬í¬íŠ¸",
            f"ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"ì´ ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡: {len(self.benchmark_history)}ê°œ",
            ""
        ]
        
        for func_name, stats in function_stats.items():
            exec_times = stats['execution_times']
            memory_usage = stats['memory_usage']
            cpu_usage = stats['cpu_usage']
            
            report_lines.extend([
                f"## {func_name}",
                f"- ì‹¤í–‰ íšŸìˆ˜: {len(exec_times)}íšŒ",
                f"- í‰ê·  ì‹¤í–‰ ì‹œê°„: {np.mean(exec_times):.3f}Â±{np.std(exec_times):.3f}ì´ˆ",
                f"- í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {np.mean(memory_usage):.2f}Â±{np.std(memory_usage):.2f}MB",
                f"- í‰ê·  CPU ì‚¬ìš©ë¥ : {np.mean(cpu_usage):.1f}Â±{np.std(cpu_usage):.1f}%",
                ""
            ])
        
        report_content = "\n".join(report_lines)
        
        # íŒŒì¼ ì €ì¥
        if output_path is None:
            output_path = self.results_dir / f"performance_report_{int(time.time())}.md"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_path}")
        return str(output_path)
    
    def plot_performance_trends(self, metric: str = 'execution_time', 
                              save_plot: bool = True) -> None:
        """
        ì„±ëŠ¥ íŠ¸ë Œë“œ ì‹œê°í™”
        
        Args:
            metric: ì‹œê°í™”í•  ë©”íŠ¸ë¦­ ('execution_time', 'memory_usage_mb', 'cpu_usage_percent')
            save_plot: í”Œë¡¯ ì €ì¥ ì—¬ë¶€
        """
        if not self.benchmark_history:
            logger.warning("ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í•¨ìˆ˜ë³„ ë°ì´í„° ìˆ˜ì§‘
        function_data = {}
        for record in self.benchmark_history:
            func_name = record['function_name']
            timestamp = record['timestamp']
            metric_value = record['metrics'].get(metric, 0)
            
            if func_name not in function_data:
                function_data[func_name] = {'timestamps': [], 'values': []}
            
            function_data[func_name]['timestamps'].append(timestamp)
            function_data[func_name]['values'].append(metric_value)
        
        # í”Œë¡¯ ìƒì„±
        plt.figure(figsize=(12, 8))
        
        for func_name, data in function_data.items():
            timestamps = [time.strftime('%H:%M:%S', time.localtime(ts)) for ts in data['timestamps']]
            plt.plot(timestamps, data['values'], marker='o', label=func_name)
        
        plt.title(f'ì„±ëŠ¥ íŠ¸ë Œë“œ: {metric}')
        plt.xlabel('ì‹œê°„')
        plt.ylabel(metric)
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        if save_plot:
            plot_path = self.results_dir / f"performance_trend_{metric}_{int(time.time())}.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            logger.info(f"ì„±ëŠ¥ íŠ¸ë Œë“œ í”Œë¡¯ ì €ì¥: {plot_path}")
        
        plt.show()
    
    def clear_history(self) -> None:
        """ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡ ì´ˆê¸°í™”"""
        self.benchmark_history.clear()
        logger.info("ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì „ì—­ ë²¤ì¹˜ë§ˆí¬ ì¸ìŠ¤í„´ìŠ¤
_benchmark = None

def get_benchmark() -> PerformanceBenchmark:
    """ì „ì—­ ë²¤ì¹˜ë§ˆí¬ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _benchmark
    if _benchmark is None:
        _benchmark = PerformanceBenchmark()
    return _benchmark

def benchmark(func_name: str = None):
    """ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„° (í¸ì˜ í•¨ìˆ˜)"""
    return get_benchmark().measure_performance(func_name) 